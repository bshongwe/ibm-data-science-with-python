{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMGPXX08RAEN86-2022-01-01\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Building a Machine Learning Pipeline For Natural Language Processing**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated time needed: **90** minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment analysis identifies the emotional tone behind a body of text, in this project we will determine the sentiment of  movie reviews. The  method  will take in a text X and return a label $yhat$ of \"1\" if the sentiment of the text is positive, \"-1\" if the sentiment of the text is negative, and \"0\" for neutral.  We star of with a rule based method, then Machine Learning method with <a href=\"https://scikit-learn.org/stable/index.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMGPXX08RAEN86-2022-01-01\">scikit-learn</a> to automatically determine the Sentiment, the dataset is take from <a href=\"https://nlp.stanford.edu/sentiment/index.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMGPXX08RAEN86-2022-01-01\">[1]</a>\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Table of Contents__\n",
    "\n",
    "<ol>\n",
    "    <li><a href=\"#Objectives\">Objectives</a></li>\n",
    "    <li>\n",
    "        <a href=\"#Setup\">Setup</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Installing-Required-Libraries\">Installing Required Libraries</a></li>\n",
    "            <li><a href=\"#Importing-Required-Libraries\">Importing Required Libraries</a></li>\n",
    "            <li><a href=\"#Defining-Helper-Functions\">Defining Helper Functions</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li>\n",
    "        <a href=\"#Background-(optional)\">Background (optional)</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#What-does-[Subject-of-the-Lab]-do?\">What does [Subject of the Lab] do?</a></li>\n",
    "            <li><a href=\"#How-does-[Subject-of-the-Lab]-work?\">How does [Subject of the Lab] work?</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li><a href=\"#Example-1---Classification-Problem.--Predicting-the-types-of-Iris-flowers\">Example 1: Classification Problem. Predicting the types of Iris flowers</a></li>\n",
    "    <li><a href=\"#Example-2---Model-Building.-Application-of-reading-cheques\">Example 2: Model Building. Application of reading cheques</a></li>\n",
    "</ol>\n",
    "\n",
    "<a href=\"#Exercises\">Exercises</a>\n",
    "<ol>\n",
    "    <li><a href=\"#Exercise-1---Loading-a-dataset-(wine-dataset)\">Exercise 1. Loading a dataset (wine dataset)</a></li>\n",
    "    <li><a href=\"#Exercise-2---Type-in-Exercise-2-subject-here\">Exercise 2. Type in Exercise 2 subject here</a></li>\n",
    "    <li><a href=\"#Exercise-3---Type-in-Exercise-3-subject-here\">Exercise 3. Type in Exercise 3 subject here</a></li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "After completing this lab you will be able to:\n",
    "\n",
    " -  Understand Sentiment analysis\n",
    " - Understand and Apply Bag-Of-Words and  Term Frequency–Inverse Document Frequency to Sentiment analysis\n",
    "  - Apply Machine Learning pipe line using  scikit-learn \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab, we will be using the following libraries:\n",
    "\n",
    "*   [`pandas`](https://pandas.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for managing the data.\n",
    "*   [`numpy`](https://numpy.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for mathematical operations.\n",
    "*   [`sklearn`](https://scikit-learn.org/stable/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for machine learning and machine-learning-pipeline related \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Required Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                  __    __    __    __\n",
      "                 /  \\  /  \\  /  \\  /  \\\n",
      "                /    \\/    \\/    \\/    \\\n",
      "███████████████/  /██/  /██/  /██/  /████████████████████████\n",
      "              /  / \\   / \\   / \\   / \\  \\____\n",
      "             /  /   \\_/   \\_/   \\_/   \\    o \\__,\n",
      "            / _/                       \\_____/  `\n",
      "            |/\n",
      "        ███╗   ███╗ █████╗ ███╗   ███╗██████╗  █████╗\n",
      "        ████╗ ████║██╔══██╗████╗ ████║██╔══██╗██╔══██╗\n",
      "        ██╔████╔██║███████║██╔████╔██║██████╔╝███████║\n",
      "        ██║╚██╔╝██║██╔══██║██║╚██╔╝██║██╔══██╗██╔══██║\n",
      "        ██║ ╚═╝ ██║██║  ██║██║ ╚═╝ ██║██████╔╝██║  ██║\n",
      "        ╚═╝     ╚═╝╚═╝  ╚═╝╚═╝     ╚═╝╚═════╝ ╚═╝  ╚═╝\n",
      "\n",
      "        mamba (1.4.2) supported by @QuantStack\n",
      "\n",
      "        GitHub:  https://github.com/mamba-org/mamba\n",
      "        Twitter: https://twitter.com/QuantStack\n",
      "\n",
      "█████████████████████████████████████████████████████████████\n",
      "\n",
      "\n",
      "Looking for: ['nltk']\n",
      "\n",
      "\u001b[?25l\u001b[2K\u001b[0G[+] 0.0s\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.1s\n",
      "pkgs/main/linux-64 \u001b[33m━━━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m   0.0 B /  ??.?MB @  ??.?MB/s  0.1s\n",
      "pkgs/main/noarch   \u001b[33m━━━━━━━━━━━━╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m   0.0 B /  ??.?MB @  ??.?MB/s  0.1s\n",
      "pkgs/r/linux-64    \u001b[33m━━━━━━━╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m   0.0 B /  ??.?MB @  ??.?MB/s  0.1s\n",
      "pkgs/r/noarch      \u001b[90m━━━━━━━━━━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━━\u001b[0m   0.0 B /  ??.?MB @  ??.?MB/s  0.1s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.2s\n",
      "pkgs/main/linux-64 \u001b[90m━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m   0.0 B /  ??.?MB @  ??.?MB/s  0.2s\n",
      "pkgs/main/noarch   \u001b[33m━━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m   0.0 B /  ??.?MB @  ??.?MB/s  0.2s\n",
      "pkgs/r/linux-64    \u001b[33m━━━━━━━━━╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m   0.0 B /  ??.?MB @  ??.?MB/s  0.2s\n",
      "pkgs/r/noarch      \u001b[90m━━━━━━━━━━━━╸\u001b[0m\u001b[33m━━━━━━━━━━━━\u001b[0m   0.0 B /  ??.?MB @  ??.?MB/s  0.2s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.3s\n",
      "pkgs/main/linux-64 \u001b[90m━━━━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━━━━\u001b[0m 258.1kB /  ??.?MB @   1.0MB/s  0.3s\n",
      "pkgs/main/noarch   \u001b[90m╸\u001b[0m\u001b[33m━━━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m 344.1kB /  ??.?MB @   1.3MB/s  0.3s\n",
      "pkgs/r/linux-64    \u001b[33m━━━━━━━━━━━━╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m 553.0kB /  ??.?MB @   2.0MB/s  0.3s\n",
      "pkgs/r/noarch      \u001b[90m━━━━━━━━━━━━━━━╸\u001b[0m\u001b[33m━━━━━━━━━\u001b[0m 503.8kB /  ??.?MB @   1.8MB/s  0.3s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.4s\n",
      "pkgs/main/linux-64 \u001b[90m━━━━━━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━━\u001b[0m 749.6kB /  ??.?MB @   2.1MB/s  0.4s\n",
      "pkgs/main/noarch   \u001b[90m━━━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━━━━━\u001b[0m 704.5kB /  ??.?MB @   1.9MB/s  0.4s\n",
      "pkgs/r/linux-64    \u001b[33m━━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m   1.1MB /  ??.?MB @   2.9MB/s  0.4s\n",
      "pkgs/r/noarch      \u001b[33m━━━━━━━━━╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m   1.0MB /  ??.?MB @   2.8MB/s  0.4s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Gpkgs/main/noarch                                   912.2kB @   2.1MB/s  0.4s\n",
      "[+] 0.5s\n",
      "pkgs/main/linux-64 \u001b[90m━━━━━━━━━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━━━\u001b[0m   1.4MB /  ??.?MB @   2.8MB/s  0.5s\n",
      "pkgs/r/linux-64    \u001b[90m━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m   1.7MB /  ??.?MB @   3.4MB/s  0.5s\n",
      "pkgs/r/noarch      \u001b[33m━━━━━━━━━━╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m   1.3MB /  ??.?MB @   3.1MB/s  0.5s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Gpkgs/r/linux-64                                      1.9MB @   3.5MB/s  0.6s\n",
      "[+] 0.6s\n",
      "pkgs/main/linux-64 \u001b[90m━━━━━━━━━━━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━\u001b[0m   1.8MB /  ??.?MB @   3.1MB/s  0.6s\n",
      "pkgs/r/noarch      \u001b[33m━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m   2.1MB /  ??.?MB @   3.6MB/s  0.6s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Gpkgs/r/noarch                                        2.3MB @   3.8MB/s  0.6s\n",
      "[+] 0.7s\n",
      "pkgs/main/linux-64 \u001b[90m━━━━━━━━━━━━╸\u001b[0m\u001b[33m━━━━━━━━━━━━\u001b[0m   2.1MB /  ??.?MB @   3.3MB/s  0.7s\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.8s\n",
      "pkgs/main/linux-64 \u001b[90m━━━━━━━━━━━━━━━╸\u001b[0m\u001b[33m━━━━━━━━━\u001b[0m   2.7MB /  ??.?MB @   3.4MB/s  0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.9s\n",
      "pkgs/main/linux-64 \u001b[33m━━━━━━━━━╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m   3.2MB /  ??.?MB @   3.6MB/s  0.9s\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.0s\n",
      "pkgs/main/linux-64 \u001b[33m━━━━━━━━━━━╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m   3.7MB /  ??.?MB @   3.8MB/s  1.0s\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.1s\n",
      "pkgs/main/linux-64 \u001b[33m━━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m   4.2MB /  ??.?MB @   3.8MB/s  1.1s\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.2s\n",
      "pkgs/main/linux-64 \u001b[90m╸\u001b[0m\u001b[33m━━━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m   4.6MB /  ??.?MB @   3.8MB/s  1.2s\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.3s\n",
      "pkgs/main/linux-64 \u001b[90m━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m   4.8MB /  ??.?MB @   3.8MB/s  1.3s\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.4s\n",
      "pkgs/main/linux-64 \u001b[90m━━━━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━━━━\u001b[0m   5.3MB /  ??.?MB @   3.9MB/s  1.4s\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.5s\n",
      "pkgs/main/linux-64 \u001b[90m━━━━━━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━━\u001b[0m   5.8MB /  ??.?MB @   4.0MB/s  1.5s\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.6s\n",
      "pkgs/main/linux-64 \u001b[90m━━━━━━━━━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━━━\u001b[0m   6.2MB /  ??.?MB @   3.9MB/s  1.6s\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.7s\n",
      "pkgs/main/linux-64 \u001b[90m━━━━━━━━━━━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━\u001b[0m   6.7MB /  ??.?MB @   4.0MB/s  1.7s\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.8s\n",
      "pkgs/main/linux-64 \u001b[90m━━━━━━━━━━━━━╸\u001b[0m\u001b[33m━━━━━━━━━━━\u001b[0m   7.1MB /  ??.?MB @   4.0MB/s  1.8s\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.9s\n",
      "pkgs/main/linux-64 \u001b[33m━━━━━━━━╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m   7.7MB /  ??.?MB @   4.1MB/s  1.9s\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.0s\n",
      "pkgs/main/linux-64 \u001b[33m━━━━━━━━━━╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m   8.2MB /  ??.?MB @   4.1MB/s  2.0s\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.1s\n",
      "pkgs/main/linux-64 \u001b[33m━━━━━━━━━━━╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m   8.5MB /  ??.?MB @   4.2MB/s  2.1s\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.2s\n",
      "pkgs/main/linux-64 ━━━━━━━━━━━━━━━━━━━━━━━━   8.6MB @   4.2MB/s Finalizing  2.2s\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.3s\n",
      "pkgs/main/linux-64 ━━━━━━━━━━━━━━━━━━━━━━━━   8.6MB @   4.2MB/s Finalizing  2.3s\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.4s\n",
      "pkgs/main/linux-64 ━━━━━━━━━━━━━━━━━━━━━━━━   8.6MB @   4.2MB/s Finalizing  2.4s\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.5s\n",
      "pkgs/main/linux-64 ━━━━━━━━━━━━━━━━━━━━━━━━   8.6MB @   4.2MB/s Finalizing  2.5s\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.6s\n",
      "pkgs/main/linux-64 ━━━━━━━━━━━━━━━━━━━━━━━━   8.6MB @   4.2MB/s Finalizing  2.6s\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.7s\n",
      "pkgs/main/linux-64 ━━━━━━━━━━━━━━━━━━━━━━━━   8.6MB @   4.2MB/s Finalizing  2.7s\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.8s\n",
      "pkgs/main/linux-64 ━━━━━━━━━━━━━━━━━━━━━━━━   8.6MB @   4.2MB/s Finalizing  2.8s\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.9s\n",
      "pkgs/main/linux-64 ━━━━━━━━━━━━━━━━━━━━━━━━   8.6MB @   4.2MB/s Finalizing  2.9s\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 3.0s\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[0Gpkgs/main/linux-64                                 @   4.2MB/s  3.0s\n",
      "\u001b[?25h\n",
      "Pinned packages:\n",
      "  - python 3.7.*\n",
      "\n",
      "\n",
      "Transaction\n",
      "\n",
      "  Prefix: /home/jupyterlab/conda/envs/python\n",
      "\n",
      "  Updating specs:\n",
      "\n",
      "   - nltk\n",
      "   - ca-certificates\n",
      "   - certifi\n",
      "   - openssl\n",
      "\n",
      "\n",
      "  Package              Version  Build           Channel                 Size\n",
      "──────────────────────────────────────────────────────────────────────────────\n",
      "  Install:\n",
      "──────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "  \u001b[32m+ joblib         \u001b[0m      1.1.1  py37h06a4308_0  pkgs/main/linux-64     388kB\n",
      "  \u001b[32m+ nltk           \u001b[0m        3.7  pyhd3eb1b0_0    pkgs/main/noarch         1MB\n",
      "  \u001b[32m+ regex          \u001b[0m   2022.7.9  py37h5eee18b_0  pkgs/main/linux-64     340kB\n",
      "\n",
      "  Upgrade:\n",
      "──────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "  \u001b[31m- ca-certificates\u001b[0m   2023.5.7  hbcca054_0      conda-forge                 \n",
      "  \u001b[32m+ ca-certificates\u001b[0m  2025.2.25  h06a4308_0      pkgs/main/linux-64     132kB\n",
      "  \u001b[31m- openssl        \u001b[0m     1.1.1t  h0b41bf4_0      conda-forge                 \n",
      "  \u001b[32m+ openssl        \u001b[0m     1.1.1w  h7f8727e_0      pkgs/main/linux-64       4MB\n",
      "\n",
      "  Summary:\n",
      "\n",
      "  Install: 3 packages\n",
      "  Upgrade: 2 packages\n",
      "\n",
      "  Total download: 6MB\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "\n",
      "\u001b[?25l\u001b[2K\u001b[0G[+] 0.0s\n",
      "Downloading      \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m   0.0 B                            0.0s\n",
      "Extracting       \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m       0                            0.0s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.1s\n",
      "Downloading  (5) \u001b[33m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m   0.0 B ca-certificates            0.0s\n",
      "Extracting       \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m       0                            0.0s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Gca-certificates                                    132.3kB @ 810.2kB/s  0.2s\n",
      "openssl                                              3.9MB @  23.7MB/s  0.2s\n",
      "joblib                                             388.5kB @   2.0MB/s  0.2s\n",
      "regex                                              339.9kB @   1.8MB/s  0.2s\n",
      "[+] 0.2s\n",
      "Downloading  (1) ━━━━━━━━━━━━━━━━━╸\u001b[33m━━━━━\u001b[0m   4.8MB nltk                       0.2s\n",
      "Extracting   (4) \u001b[90m╸\u001b[0m\u001b[33m━━━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━━━━━━\u001b[0m       0 ca-certificates            0.0s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Gnltk                                                 1.0MB @   3.8MB/s  0.3s\n",
      "[+] 0.3s\n",
      "Downloading      ━━━━━━━━━━━━━━━━━━━━━━━   5.8MB                            0.2s\n",
      "Extracting   (5) \u001b[90m━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━━━━━\u001b[0m       0 ca-certificates            0.0s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.4s\n",
      "Downloading      ━━━━━━━━━━━━━━━━━━━━━━━   5.8MB                            0.2s\n",
      "Extracting   (5) \u001b[90m━━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━━━━\u001b[0m       0 ca-certificates            0.1s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.5s\n",
      "Downloading      ━━━━━━━━━━━━━━━━━━━━━━━   5.8MB                            0.2s\n",
      "Extracting   (5) \u001b[90m━━━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━━━\u001b[0m       0 ca-certificates            0.2s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.6s\n",
      "Downloading      ━━━━━━━━━━━━━━━━━━━━━━━   5.8MB                            0.2s\n",
      "Extracting   (5) \u001b[90m━━━━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━━\u001b[0m       0 joblib                     0.3s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.7s\n",
      "Downloading      ━━━━━━━━━━━━━━━━━━━━━━━   5.8MB                            0.2s\n",
      "Extracting   (5) \u001b[90m━━━━━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━\u001b[0m       0 joblib                     0.4s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.8s\n",
      "Downloading      ━━━━━━━━━━━━━━━━━━━━━━━   5.8MB                            0.2s\n",
      "Extracting   (5) \u001b[90m━━━━━━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━━━━\u001b[0m       0 joblib                     0.5s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.9s\n",
      "Downloading      ━━━━━━━━━━━━━━━━━━━━━━━   5.8MB                            0.2s\n",
      "Extracting   (5) \u001b[90m━━━━━━━━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━━\u001b[0m       0 joblib                     0.6s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.0s\n",
      "Downloading      ━━━━━━━━━━━━━━━━━━━━━━━   5.8MB                            0.2s\n",
      "Extracting   (5) \u001b[90m━━━━━━━━━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━\u001b[0m       0 nltk                       0.7s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.1s\n",
      "Downloading      ━━━━━━━━━━━━━━━━━━━━━━━   5.8MB                            0.2s\n",
      "Extracting   (5) \u001b[90m━━━━━━━━━━╸\u001b[0m\u001b[33m━━━━━━━━━━━━\u001b[0m       0 nltk                       0.9s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.2s\n",
      "Downloading      ━━━━━━━━━━━━━━━━━━━━━━━   5.8MB                            0.2s\n",
      "Extracting   (5) \u001b[90m━━━━━━━━━━━╸\u001b[0m\u001b[33m━━━━━━━━━━━\u001b[0m       0 nltk                       1.0s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.3s\n",
      "Downloading      ━━━━━━━━━━━━━━━━━━━━━━━   5.8MB                            0.2s\n",
      "Extracting   (5) \u001b[90m━━━━━━━━━━━━╸\u001b[0m\u001b[33m━━━━━━━━━━\u001b[0m       0 nltk                       1.0s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.4s\n",
      "Downloading      ━━━━━━━━━━━━━━━━━━━━━━━   5.8MB                            0.2s\n",
      "Extracting   (5) \u001b[33m━━━━━━━╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m       0 openssl                    1.1s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.5s\n",
      "Downloading      ━━━━━━━━━━━━━━━━━━━━━━━   5.8MB                            0.2s\n",
      "Extracting   (5) \u001b[33m━━━━━━━━╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m       0 openssl                    1.2s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.6s\n",
      "Downloading      ━━━━━━━━━━━━━━━━━━━━━━━   5.8MB                            0.2s\n",
      "Extracting   (5) \u001b[33m━━━━━━━━━╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m       0 openssl                    1.3s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.7s\n",
      "Downloading      ━━━━━━━━━━━━━━━━━━━━━━━   5.8MB                            0.2s\n",
      "Extracting   (5) \u001b[33m━━━━━━━━━━╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m       0 openssl                    1.4s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.8s\n",
      "Downloading      ━━━━━━━━━━━━━━━━━━━━━━━   5.8MB                            0.2s\n",
      "Extracting   (5) \u001b[33m━━━━━━━━━━━╸\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m       0 regex                      1.5s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.9s\n",
      "Downloading      ━━━━━━━━━━━━━━━━━━━━━━━   5.8MB                            0.2s\n",
      "Extracting   (5) \u001b[33m━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m       0 regex                      1.6s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.0s\n",
      "Downloading      ━━━━━━━━━━━━━━━━━━━━━━━   5.8MB                            0.2s\n",
      "Extracting   (5) \u001b[33m━━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m       0 regex                      1.7s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.1s\n",
      "Downloading      ━━━━━━━━━━━━━━━━━━━━━━━   5.8MB                            0.2s\n",
      "Extracting   (5) \u001b[33m━━━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m       0 regex                      1.8s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.2s\n",
      "Downloading      ━━━━━━━━━━━━━━━━━━━━━━━   5.8MB                            0.2s\n",
      "Extracting   (5) \u001b[90m╸\u001b[0m\u001b[33m━━━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━━━━━━\u001b[0m       0 ca-certificates            1.9s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.3s\n",
      "Downloading      ━━━━━━━━━━━━━━━━━━━━━━━   5.8MB                            0.2s\n",
      "Extracting   (5) \u001b[90m━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━━━━━\u001b[0m       0 ca-certificates            2.0s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.4s\n",
      "Downloading      ━━━━━━━━━━━━━━━━━━━━━━━   5.8MB                            0.2s\n",
      "Extracting   (5) \u001b[90m━━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━━━━\u001b[0m       0 ca-certificates            2.1s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.5s\n",
      "Downloading      ━━━━━━━━━━━━━━━━━━━━━━━   5.8MB                            0.2s\n",
      "Extracting   (4) ━━━╸\u001b[33m━━━━━━━━━━━━━━━━━━━\u001b[0m       1 joblib                     2.2s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.6s\n",
      "Downloading      ━━━━━━━━━━━━━━━━━━━━━━━   5.8MB                            0.2s\n",
      "Extracting   (3) ━━━━━━━━╸\u001b[33m━━━━━━━━━━━━━━\u001b[0m       2 joblib                     2.3s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.7s\n",
      "Downloading      ━━━━━━━━━━━━━━━━━━━━━━━   5.8MB                            0.2s\n",
      "Extracting   (2) ━━━━━━━━━━━━╸\u001b[33m━━━━━━━━━━\u001b[0m       3 nltk                       2.4s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.8s\n",
      "Downloading      ━━━━━━━━━━━━━━━━━━━━━━━   5.8MB                            0.2s\n",
      "Extracting   (1) ━━━━━━━━━━━━━━━━━╸\u001b[33m━━━━━\u001b[0m       4 nltk                       2.5s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.9s\n",
      "Downloading      ━━━━━━━━━━━━━━━━━━━━━━━   5.8MB                            0.2s\n",
      "Extracting       ━━━━━━━━━━━━━━━━━━━━━━━       5                            2.6s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G\u001b[?25h\n",
      "Downloading and Extracting Packages\n",
      "\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "# All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented.\n",
    "# !mamba install -qy pandas==1.3.4 numpy==1.21.4 seaborn==0.9.0 matplotlib==3.5.0 scikit-learn==0.20.1\n",
    "# Note: If your environment doesn't support \"!mamba install\", use \"!pip install\"\n",
    "!mamba install nltk --y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Required Libraries\n",
    "\n",
    "_We recommend you import all required libraries in one place (here):_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import warnings\n",
    "\n",
    "# Specifically target the DeprecationWarning related to np.float and distutils\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning, \n",
    "                       message=\".*np.float is a deprecated alias.*\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning,\n",
    "                       message=\".*distutils Version classes are deprecated.*\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from scipy.sparse import dok_matrix\n",
    "# You can also use this section to suppress warnings generated by your code:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')\n",
    "warn()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "warn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Helper Functions\n",
    "\n",
    "This function will print the class and document dataframe \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_sample_and_class(data, class_=1,n_samples=5):\n",
    "\n",
    "    index=data[data['y']==class_]['X'].index[0:n_samples]\n",
    "\n",
    "    for i in index:\n",
    "        print(\"sample {} of class {}\".format(i,class_))\n",
    "        print(data[data['y']==class_]['X'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rule Based Natural Language Processing\n",
    "\n",
    "Intuitively you would calculate the sentiment of a document by counting the words you associate with a good, neutral or lousy sentiment. This is how rule-based NLP systems work; they follow pre-determined rules to categorize the language it's analyzing. This section will review some basic string operations in pandas and determine the number of predefined words belonging to a specific sentiment to determine the sentiment of a document. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load our text data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>X</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The Rock is destined to be the 21st Century '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The gorgeously elaborate continuation of `` T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Singer\\/composer Bryan Adams contributes a sl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>You 'd think by now America would have had en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Yet the act is still charming here .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   y                                                  X\n",
       "0  1   The Rock is destined to be the 21st Century '...\n",
       "1  1   The gorgeously elaborate continuation of `` T...\n",
       "2  1   Singer\\/composer Bryan Adams contributes a sl...\n",
       "3  0   You 'd think by now America would have had en...\n",
       "4  1               Yet the act is still charming here ."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=  pd.read_csv(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-GPXX08RAEN/sentiment-text-threeclass/train.txt\",header=None, sep=\"\\\\|\\\\|\\\\|\",names=['y','X'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the column ```y``` sentiment and the column ```X``` is the document or sample is the we print the first five sample documents  with the  sentiment and the document:\n",
    "\n",
    "\"1\" if the sentiment of the text is positive\n",
    "\n",
    "\"-1\" if the sentiment of the text is negative\n",
    "\n",
    "\"0\" for neutral  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 0 of class 1\n",
      " The Rock is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .\n",
      "sample 1 of class 1\n",
      " The gorgeously elaborate continuation of `` The Lord of the Rings '' trilogy is so huge that a column of words can not adequately describe co-writer\\/director Peter Jackson 's expanded vision of J.R.R. Tolkien 's Middle-earth .\n",
      "sample 2 of class 1\n",
      " Singer\\/composer Bryan Adams contributes a slew of songs -- a few potential hits , a few more simply intrusive to the story -- but the whole package certainly captures the intended , er , spirit of the piece .\n",
      "sample 4 of class 1\n",
      " Yet the act is still charming here .\n",
      "sample 5 of class 1\n",
      " Whether or not you 're enlightened by any of Derrida 's lectures on `` the other '' and `` the self , '' Derrida is an undeniably fascinating and playful fellow .\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print_sample_and_class(df, class_=1,n_samples=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each sample in the column \"X\" corresponds to a Python string \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document:  The Rock is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .\n",
      "\n",
      " type: <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "my_string=df['X'][0]\n",
    "print(\"document:\",my_string)\n",
    "print(\"\\n type:\",type(my_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print out the first 10 characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The Rock '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_string[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can split the string into a individual  of words, each word is called a <b>token</b> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Rock',\n",
       " 'is',\n",
       " 'destined',\n",
       " 'to',\n",
       " 'be',\n",
       " 'the',\n",
       " '21st',\n",
       " 'Century',\n",
       " \"'s\",\n",
       " 'new',\n",
       " '``',\n",
       " 'Conan',\n",
       " \"''\",\n",
       " 'and',\n",
       " 'that',\n",
       " 'he',\n",
       " \"'s\",\n",
       " 'going',\n",
       " 'to',\n",
       " 'make',\n",
       " 'a',\n",
       " 'splash',\n",
       " 'even',\n",
       " 'greater',\n",
       " 'than',\n",
       " 'Arnold',\n",
       " 'Schwarzenegger',\n",
       " ',',\n",
       " 'Jean-Claud',\n",
       " 'Van',\n",
       " 'Damme',\n",
       " 'or',\n",
       " 'Steven',\n",
       " 'Segal',\n",
       " '.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_string.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In python, we can apply different string operations like converting each character  to lowercase:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" the rock is destined to be the 21st century 's new `` conan '' and that he 's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_string.casefold()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply this operation to every column by using .str attribute followed by the string operations we would like to perform. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        the rock is destined to be the 21st century '...\n",
       "1        the gorgeously elaborate continuation of `` t...\n",
       "2        singer\\/composer bryan adams contributes a sl...\n",
       "3        you 'd think by now america would have had en...\n",
       "4                    yet the act is still charming here .\n",
       "                              ...                        \n",
       "8539                                      a real snooze .\n",
       "8540                                       no surprises .\n",
       "8541     we 've seen the hippie-turned-yuppie plot bef...\n",
       "8542     her fans walked out muttering words like `` h...\n",
       "8543                                  in this case zero .\n",
       "Name: X, Length: 8544, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"X\"].str.casefold()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have a list of positive and negative words \\:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " # Count the number of \"good words\" and \"bad words\" in the text\n",
    "good_words = ['love', 'good','excellent', 'great','charming']\n",
    "\n",
    "\n",
    "bad_words = ['hate', 'bad','brutal', 'damnable', 'deplorable', 'detestable', 'disastrous', 'dreadful']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assign a 1 to the positive sentiment word and a -1  to all the negative sentiment words, then calculate the total for each document and place it in the column score:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"score\"]=0\n",
    "for bad_word in bad_words:\n",
    "    df[\"score\"]-=df[\"X\"].str.casefold().str.count(bad_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "for good_words in good_words:\n",
    "    df[\"score\"]+=df[\"X\"].str.casefold().str.count(good_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are giving each word a score, one for a  positive sentiment word and a negative one for negative sentiment word. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>X</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The Rock is destined to be the 21st Century '...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The gorgeously elaborate continuation of `` T...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Singer\\/composer Bryan Adams contributes a sl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>You 'd think by now America would have had en...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Yet the act is still charming here .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   y                                                  X  score\n",
       "0  1   The Rock is destined to be the 21st Century '...      1\n",
       "1  1   The gorgeously elaborate continuation of `` T...      0\n",
       "2  1   Singer\\/composer Bryan Adams contributes a sl...      0\n",
       "3  0   You 'd think by now America would have had en...      0\n",
       "4  1               Yet the act is still charming here .      1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can print out the document with the highest score:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Good fun , good action , good acting , good dialogue , good pace , good cinematography .'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df[\"score\"].argmax(axis=0),'X']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can also print out the document with the lowest score:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' It is that rare combination of bad writing , bad direction and bad acting -- the trifecta of badness .'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df[\"score\"].argmin(axis=0),'X']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the score for positive and negative and neural classes \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>0.003021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.057882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.106094</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       score\n",
       "y           \n",
       "-1  0.003021\n",
       " 0  0.057882\n",
       " 1  0.106094"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('y').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we see that the scores are relatively the same. We can also plot a histogram we see for each class most of the samples overlap.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "y\n",
       "-1    AxesSubplot(0.125,0.11;0.775x0.77)\n",
       " 0    AxesSubplot(0.125,0.11;0.775x0.77)\n",
       " 1    AxesSubplot(0.125,0.11;0.775x0.77)\n",
       "Name: score, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[[\"score\",\"y\"]].groupby('y')[\"score\"].hist(legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new column ```yhat``` for the sentiment we will assign each sample to a neutral sentiment:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['yhat']=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can set rule-based thresholds if the score is less than the threshold; we set the sentiment to positive sentiment; similarly, if it's less than the negative of the threshold, we set the sentiment to negative. Anything between we set to neutral. In this case, the threshold is one; you can try different values for thresholds or different rules on your own ;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "negative_label=df[['score']].sum(axis=1)<1\n",
    "pos_label=df[['score']].sum(axis=1)>-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assign the class according the the score:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['yhat'][negative_label]=-1\n",
    "df['yhat'][pos_label]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    3610\n",
       "-1    3310\n",
       " 0    1624\n",
       "Name: y, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['y'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare the predicted sentiment with the actual  sentiment, we see the accuracy is not good\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43562734082397003"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(df['yhat']==df['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many reasons why the rule based classifier did not work. The list of words was small the datasets can have thousands of words both positive and negative words; also  we dint have any neutral words. Are scoring methods we use gives a  one for a  positive sentiment word and a negative one for negative sentiment word, maybe some words should be grater then one for  positive sentiment like \"amazing\" should be 10 and ok  should be 4, similarly some words with negative  sentiment should have large negative values like \"barfed \" should be \"-10\" and \"boring\" should be -4. Maybe  neutral and negative words may have a score for positive sentiment and vise versa. One way to answer these questions is to use machine learning to determine these these scores. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "Machine Learning (ML) is a standard tool for NLP tasks. There are many machine learning methods, and in addition, there are many machine learning methods specially built for NLP tasks. We will use multi-class logistic regression via <a href=\"https://scikit-learn.org/stable/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMGPXX08RAEN86-2022-01-01\"> scikit-learn</a> , a more general approach. We will for two reasons ; first, if you have any experience in Machine learning, you probably have experience with  logistic regression, second  logistic regression can be interpreted as a kind of scoring. The  parameters in this case the scores  are obtained via training or learning, we try different scores called weights for each word until we minimize the miss classified samples. This is done in an optimal way.  It turns out that ML can classify NLP data so well we have to test our method using data we haven seen, we actually have to test it twice on data it has not see, the datasets form \n",
    "\n",
    "\n",
    "<b>Training data set</b>\n",
    "A training data set is a data set of examples used during the learning process and is used to fit the parameters (e.g., weights or scores ) \n",
    "\n",
    "\n",
    "\n",
    "<b>Validation data set</b>\n",
    "A validation data set is a data-set of examples used to tune the hyperparameters  these are related to the learning training and are chose by experimenting \n",
    "\n",
    "<b>Test data set</b>\n",
    "A test data set is a data set that is independent of the training data set and Validation , it basicly how good you model should do in the real world \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset=pd.read_csv(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-GPXX08RAEN/sentiment-text-threeclass/train.txt\",header=None, sep=\"\\\\|\\\\|\\\\|\",names=['y','X']) \n",
    "validation_dataset=pd.read_csv(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-GPXX08RAEN/sentiment-text-threeclass/dev.txt\",header=None, sep=\"\\\\|\\\\|\\\\|\",names=['y','X']) \n",
    "test_dataset =pd.read_csv(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-GPXX08RAEN/sentiment-text-threeclass/test.txt\",header=None, sep=\"\\\\|\\\\|\\\\|\",names=['y','X']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " \n",
    "Machines, different from humans, cannot comprehend raw text.  Therefore, we have got to transform our content into numbers we denote features for $n-th $ document with the feature vector $\\mathbf{x}_n$. The $i-th$ word would be the i-th element in $\\mathbf{x}_n$ denoted by  $x_i$ we drop the $n$ for simplicity. The dataset would be the matrix $\\mathbf{X}$  where each row corresponds to a document and each column  corresponds to a word  .Lets see how to transform documents into corresponding numerical features:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-Of-Words \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bag-Of-Words (BoW) model transforms text into fixed-length vectors; for example, a count if the number of times the word is present in a document or token counts. Scikit-learn's ```CountVectorizer```  is one method to perform BoW transformation; it converts a set of text documents to a matrix of token counts called a document-term matrix (TDM). \n",
    "The TDM is a sparse matrix object   ```scipy.sparse.csr_matrix```; where each row represents a different document, and each column represents each word in the document. For each  element in the matrix, represent how many times that word occurs. First we import  ```CountVectorizer``` object:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy Example \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's test it the the following toy corpus:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " corpus = [\n",
    "    'This is the first of document .',\n",
    "    'This document is the second document.',\n",
    "     'And this is the third one.',\n",
    "     'Is this the first document?' ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we apply the ```CountVectorizer``` object,the output is the TDM a sparse matrix object:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x10 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 22 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer=CountVectorizer()\n",
    "\n",
    "X_toy = vectorizer.fit_transform(corpus)\n",
    "X_toy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can output the word corresponding to each column \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and',\n",
       " 'document',\n",
       " 'first',\n",
       " 'is',\n",
       " 'of',\n",
       " 'one',\n",
       " 'second',\n",
       " 'the',\n",
       " 'third',\n",
       " 'this']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer. get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can cast the output to a numpy array \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 1, 0, 0, 1, 0, 1],\n",
       "       [0, 2, 0, 1, 0, 0, 1, 1, 0, 1],\n",
       "       [1, 0, 0, 1, 0, 1, 0, 1, 1, 1],\n",
       "       [0, 1, 1, 1, 0, 0, 0, 1, 0, 1]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_toy.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can convert the output to a datafame. We see the word corresponding to each column and the phrase for each row. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>document</th>\n",
       "      <th>first</th>\n",
       "      <th>is</th>\n",
       "      <th>of</th>\n",
       "      <th>one</th>\n",
       "      <th>second</th>\n",
       "      <th>the</th>\n",
       "      <th>third</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>This is the first of document .</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>This document is the second document.</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>And this is the third one.</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Is this the first document?</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       and  document  first  is  of  one  \\\n",
       "This is the first of document .          0         1      1   1   1    0   \n",
       "This document is the second document.    0         2      0   1   0    0   \n",
       "And this is the third one.               1         0      0   1   0    1   \n",
       "Is this the first document?              0         1      1   1   0    0   \n",
       "\n",
       "                                       second  the  third  this  \n",
       "This is the first of document .             0    1      0     1  \n",
       "This document is the second document.       1    1      0     1  \n",
       "And this is the third one.                  0    1      1     1  \n",
       "Is this the first document?                 0    1      0     1  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_toy_df= pd.DataFrame(X_toy.toarray(),columns=vectorizer.get_feature_names(),index=corpus )\n",
    "original_toy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first document  'This is the first of document .'  would have the featuer $\\mathbf{x}_1=[0, 1, 1, 1, 1, 0, 0, 1, 0, 1]$, the second document would be $\\mathbf{x}_2=[0, 2, 0, 1, 0, 0, 1, 1, 0, 1]$ and do on\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Model with Grid Search and Logistic Regression \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, let's train a model with Grid Serch and Logistic Regression  first we import the class constructors, you will do a much more simple example in Question 2 where you use default settings of  Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import PredefinedSplit, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is pre-split into training and validation data. As we want to determine hyperparameters automatically using  ```GridSearchCV``` we need to combine the datasets together \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset=pd.concat([train_dataset,validation_dataset],axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can keep track of what samples belong to training a validation by creating a NumPy array where negative ones correspond to training samples, and zeros correspond to validation data. We then use  a predefined scheme using ```PredefinedSplit``` .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import PredefinedSplit\n",
    "\n",
    "split_index = [-1]*train_dataset.shape[0] + [0]*validation_dataset.shape[0]\n",
    "pds = PredefinedSplit(test_fold = split_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we create a ```CountVectorizer()``` object and transform the dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(dataset['X'])\n",
    "y=dataset['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we do the same for the test dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_test=vectorizer.transform(test_dataset['X'])\n",
    "y_test=test_dataset[['y']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a dictionary of hyperparameters, the Inverse of regularization strength; and the norm of the penalty. We assume L1 would be better as the data is sparse:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "param_grid = {'penalty' : ['l1', 'l2'],'C' : np.logspace(-4, 4, 20)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a We create a ```GridSearchCV``` object , with a logistic regression estimator, the parameter ```cv``` Determines the cross-validation splitting strategy, we use the predefined split. Finely we use the the parameter grid defined above:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ...,  0,  0])),\n",
       "       error_score='raise-deprecating',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'penalty': ['l1', 'l2'], 'C': array([1.00000e-04, 2.63665e-04, 6.95193e-04, 1.83298e-03, 4.83293e-03,\n",
       "       1.27427e-02, 3.35982e-02, 8.85867e-02, 2.33572e-01, 6.15848e-01,\n",
       "       1.62378e+00, 4.28133e+00, 1.12884e+01, 2.97635e+01, 7.84760e+01,\n",
       "       2.06914e+02, 5.45559e+02, 1.43845e+03, 3.79269e+03, 1.00000e+04])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GridSearchCV(estimator = LogisticRegression(),cv=pds,param_grid=param_grid)\n",
    "clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit the model and print out the highest accuracy in the validation data and the corresponding best parameter:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "highest accuracy in the validation accuracy  0.6185286103542235\n",
      "best hyperparameters: {'C': 0.23357214690901212, 'penalty': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "clf.fit(X, y)\n",
    "print(\"highest accuracy in the validation accuracy \",clf.best_score_)\n",
    "print(\"best hyperparameters:\",clf.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also find the accuracy using the test data  we see it's much higher than the validation data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " best parameters accuracy score : 0.6493212669683258\n"
     ]
    }
   ],
   "source": [
    "print(\" best parameters accuracy score :\",clf.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can find the best model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model=clf.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and make a prediction \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  1, ...,  1, -1, -1])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "each sample corresponds the predicted class or predicted sentiment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out the logistic regression is calculating the score using parameters  $w_{ij}$ for the $j$ class\n",
    "\n",
    "$s_j= \\sum_{i=1}^{N} w_{ij}x_i$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The larger $s_j$, the higher the score, and the more likely the sample belongs to that class. As $x_i$ is usually one, a significant $w_{ij}$ means that word is more likely to contribute to that class; if $w_{ij}$ is negative, the more likely that word is not to belong to that class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We can determine the words with the largest parameters or score, for each class $j$  we sort the index of each parameter $w_{ij}$ from largest to smallest \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_importance=np.argsort(model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find the corresponding vector $x_i$ and then using ```get_feature_names_out()``` to find the word with the highest score \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(j) or y= 0\n",
      "['solid' 'best' 'powerful' 'enjoyable' 'fun' 'entertaining' 'hilarious'\n",
      " 'always' 'human' 'charming']\n",
      "(j) or y= 1\n",
      "['screen' 'offers' 'bad' 'crafted' 'imagine' 'watching' 'going'\n",
      " 'thoroughly' 'humor' 'impressive']\n",
      "(j) or y= 2\n",
      "['dull' 'worst' 'bad' 'too' 'suffers' 'mess' 'less' 'flat' 'feels' 'lack']\n"
     ]
    }
   ],
   "source": [
    "for class_ in range(3):\n",
    "    \n",
    "    print(\"(j) or y=\",class_)\n",
    "    print(np.array(vectorizer.get_feature_names())[feature_importance[class_,0:10]])\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 1 </b>  Create you own ```CountVectorizer``` function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 1 a) </b> Create a function that takes the input column of a dataframe ```train_dataset[\"X\"]``` . The  output is a dictionary where each key is a word in the corpus, and the value is a unique digit, then apply the function and call the output ```word_to_idx```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_feature_map(X):\n",
    "\n",
    "    word_types =set()\n",
    "    #Split string into words usig split() then apply(set) \n",
    "    for x in X.str.casefold().str.split().apply(set):\n",
    " \n",
    "    \n",
    "        word_types=word_types.union(x)\n",
    "    \n",
    "    # Create a dictionary keyed by word mapping it to an index\n",
    "    return   {word: idx for idx, word in enumerate(word_types)}\n",
    "\n",
    "word_to_idx = build_feature_map(train_dataset[\"X\"].str.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "def build_feature_map(X):\n",
    "\n",
    "    word_types =set()\n",
    "    #Split string into words usig split() then apply(set) \n",
    "    for x in X.str.casefold().str.split().apply(set):\n",
    " \n",
    "    \n",
    "        word_types=word_types.union(x)\n",
    "    \n",
    "    # Create a dictionary keyed by word mapping it to an index\n",
    "    return   {word: idx for idx, word in enumerate(word_types)}\n",
    "\n",
    "word_to_idx = build_feature_map(train_dataset[\"X\"]\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can use the following to map the index to the key and the word to the value ```idx_to_word_func = lambda word_to_idx :{v:k for k,v in word_to_idx.items()}```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 1 b)</b> write a function that outputs a  TDM given ```train_dataset[\"X\"]``` you can use ```word_to_idx``` as an input, apply the function to ```train_dataset[\"X\"]``` call the result X_train:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_features(word_to_idx, X):\n",
    "    D=len(word_to_idx)\n",
    "    N=X.shape[0]\n",
    "    words=set(word_to_idx.keys())\n",
    "    \n",
    "    features = dok_matrix((N, D))\n",
    "    for row,x in enumerate(X[0:]):\n",
    "        for word in x.split():\n",
    "            if word in words:\n",
    "                features[row,word_to_idx[word.casefold()]]+=1\n",
    "    return features\n",
    "\n",
    "X_train= extract_features(word_to_idx, train_dataset[\"X\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "def extract_features(word_to_idx, X):\n",
    "    D=len(word_to_idx)\n",
    "    N=X.shape[0]\n",
    "    words=set(word_to_idx.keys())\n",
    "    \n",
    "    features = dok_matrix((N, D))\n",
    "    for row,x in enumerate(X[0:]):\n",
    "        for word in x.split():\n",
    "            if word in words:\n",
    "                features[row,word_to_idx[word.casefold()]]+=1\n",
    "    return features\n",
    "\n",
    "X_train= extract_features(word_to_idx, train_dataset[\"X\"])\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 2</b> Create a logistic regression object and train it using the training data. Calculate the score using the validation data and test data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation accuracy  0.6012715712988193\n",
      "test accuracy  0.6389140271493212\n"
     ]
    }
   ],
   "source": [
    "X_train= extract_features(word_to_idx, train_dataset[\"X\"])\n",
    "y_train=train_dataset[[\"y\"]]\n",
    "\n",
    "X_val=extract_features(word_to_idx, validation_dataset[\"X\"])\n",
    "y_val=validation_dataset[['y']]\n",
    "X_test=extract_features(word_to_idx, test_dataset[\"X\"])\n",
    "y_test=test_dataset[['y']]\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "print(\"validation accuracy \",lr.score(X_val,y_val))\n",
    "print(\"test accuracy \",lr.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "X_train= extract_features(word_to_idx, train_dataset[\"X\"])\n",
    "y_train=train_dataset[[\"y\"]]\n",
    "\n",
    "X_val=extract_features(word_to_idx, validation_dataset[\"X\"])\n",
    "y_val=validation_dataset[['y']]\n",
    "X_test=extract_features(word_to_idx, test_dataset[\"X\"])\n",
    "y_test=test_dataset[['y']]\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "print(\"validation accuracy \",lr.score(X_val,y_val))\n",
    "print(\"test accuracy \",lr.score(X_test,y_test))\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-Of-Word Hyperparameters \n",
    "\n",
    "Bag-Of-Word has several  hyperparameters you can change to improve performance; most have to do with reducing the number of dimensions in the TDM this improves performance,  lets review some\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop words, which include \"and,\" \"the,\" and \"his,\" are seen to be uninformative in describing the content of a document and may be eliminated sometimes to improve performance. However, removing stop words does not always help with performance, so the  validation data should be used to determine their effectiveness. A list of stop words are available using  Natural Language Toolkit (nltk) we can download the list of stop words as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We specify the languish as English. The result is a list of stop words; we can print out the first ten:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an']\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('english')[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can use the toy data example, we add the list of stop words to the parameter  ```stop_words``` and transform  the data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words=stopwords.words('english'))\n",
    "X_toy = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare the TDM with the original TDM with no stop words we see the columns corresponding to ```and```, ```the``` and  ```this```  are missing: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>first</th>\n",
       "      <th>one</th>\n",
       "      <th>second</th>\n",
       "      <th>third</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>This is the first of document .</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>This document is the second document.</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>And this is the third one.</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Is this the first document?</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       document  first  one  second  third\n",
       "This is the first of document .               1      1    0       0      0\n",
       "This document is the second document.         2      0    0       1      0\n",
       "And this is the third one.                    0      0    1       0      1\n",
       "Is this the first document?                   1      1    0       0      0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_toy_df= pd.DataFrame(X_toy.toarray(),columns=vectorizer.get_feature_names(),index=corpus )\n",
    "new_toy_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6502262443438914"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_toy_df.head()\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=stopwords.words('english'))\n",
    "X = vectorizer.fit_transform(dataset['X'])\n",
    "X_test=vectorizer.transform(test_dataset['X'])\n",
    "y_test=test_dataset[['y']]\n",
    "\n",
    "clf = GridSearchCV(estimator = LogisticRegression(),cv=pds,param_grid=param_grid)\n",
    "\n",
    "# Fit with all data\n",
    "clf.fit(X, y)\n",
    "clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <b>Exercise </b> - Train a linear regression model using  GridSearchCV  after performing a BoW transform. Find the accuracy on the test data?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "vectorizer = CountVectorizer(stop_words=stopwords.words('english'))\n",
    "X = vectorizer.fit_transform(dataset['X'])\n",
    "X_test=vectorizer.transform(test_dataset['X'])\n",
    "y_test=test_dataset[['y']]\n",
    "\n",
    "clf = GridSearchCV(estimator = LogisticRegression(),cv=pds,param_grid=param_grid)\n",
    "\n",
    "# Fit with all data\n",
    "clf.fit(X, y)\n",
    "clf.score(X_test,y_test)\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next parameter is ```min_df```, which has been set to 5. This corresponds to the minimum number of documents that should contain this feature to be counted. Here we set ```min_df``` to two; this means at least two documents contain the word if it's to be included as a column in the TFM. The reasoning is that if very few documents have the word, it will just add an extra dimension to the TFM  and not be useful for classification. Let's apply it to our toy example and compare it to our original TFM \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "min_df=2\n",
    "vectorizer = CountVectorizer(min_df=min_df)\n",
    "X_toy = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>first</th>\n",
       "      <th>is</th>\n",
       "      <th>the</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>This is the first of document .</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>This document is the second document.</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>And this is the third one.</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Is this the first document?</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       document  first  is  the  this\n",
       "This is the first of document .               1      1   1    1     1\n",
       "This document is the second document.         2      0   1    1     1\n",
       "And this is the third one.                    0      0   1    1     1\n",
       "Is this the first document?                   1      1   1    1     1"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_toy_df= pd.DataFrame(X_toy.toarray(),columns=vectorizer.get_feature_names(),index=corpus )\n",
    "new_toy_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>document</th>\n",
       "      <th>first</th>\n",
       "      <th>is</th>\n",
       "      <th>of</th>\n",
       "      <th>one</th>\n",
       "      <th>second</th>\n",
       "      <th>the</th>\n",
       "      <th>third</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>This is the first of document .</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>This document is the second document.</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>And this is the third one.</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Is this the first document?</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       and  document  first  is  of  one  \\\n",
       "This is the first of document .          0         1      1   1   1    0   \n",
       "This document is the second document.    0         2      0   1   0    0   \n",
       "And this is the third one.               1         0      0   1   0    1   \n",
       "Is this the first document?              0         1      1   1   0    0   \n",
       "\n",
       "                                       second  the  third  this  \n",
       "This is the first of document .             0    1      0     1  \n",
       "This document is the second document.       1    1      0     1  \n",
       "And this is the third one.                  0    1      1     1  \n",
       "Is this the first document?                 0    1      0     1  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_toy_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that ```one```, ```second```\t and ```third``` are not included as the new TFM as only one document. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, if  words are contained in most documents, they may not be useful for classification. Therefore we use  for the ```max_df```, that determines what fraction or number of documents contain a word for it to be disregarded as a column int the TFM . For example if the value is set to 0.9  we should include only those words that occur in a maximum of 90% of all the documents. If the Words  occur in  90% of the documents it's usually not suitable for classification because they do not provide any unique information about the document. Here we set  ```max_df=0.75```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_df=0.75\n",
    "vectorizer = CountVectorizer(max_df=0.75)\n",
    "X_toy = vectorizer.fit_transform(corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we see words like ```is```, ```of``` and ```are ```left out of the TFM:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>document</th>\n",
       "      <th>first</th>\n",
       "      <th>of</th>\n",
       "      <th>one</th>\n",
       "      <th>second</th>\n",
       "      <th>third</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>This is the first of document .</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>This document is the second document.</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>And this is the third one.</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Is this the first document?</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       and  document  first  of  one  second  \\\n",
       "This is the first of document .          0         1      1   1    0       0   \n",
       "This document is the second document.    0         2      0   0    0       1   \n",
       "And this is the third one.               1         0      0   0    1       0   \n",
       "Is this the first document?              0         1      1   0    0       0   \n",
       "\n",
       "                                       third  \n",
       "This is the first of document .            0  \n",
       "This document is the second document.      0  \n",
       "And this is the third one.                 1  \n",
       "Is this the first document?                0  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_toy_df= pd.DataFrame(X_toy.toarray(),columns=vectorizer.get_feature_names(),index=corpus )\n",
    "new_toy_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer Gridserch \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the ``` CountVectorizer``` has its own set of hyperparameters. We can use ```GridserchCV``` to find them by first creating a pipeline  object. The pipeline object  Sequentially apply a list of transforms and a final estimator. Intermediate steps of the pipeline is a ``` CountVectorizer```,the final estimator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter steps takes a  list of tuples of the transforms and a final estimator in sequential order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[(\"CV\", CountVectorizer()),  (\"LR\", LogisticRegression())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a dictionary of hyperparameters; the transform or estimator is the first element of the tuple, then double underscore with the name of the hyperparameter we would like to change. This is followed by the values we would like to try. We try different values for ```min_df```, ```max_df``` and different amount of stop words:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"CV__stop_words\":[stopwords.words('english')[0:n] for n in range(1,150,50)],\n",
    "    \"CV__min_df\":[5**n for n in range(5)],\n",
    "    \"CV__max_df\":[0.6,0.9],\n",
    "    \"LR__penalty\":[\"l1\", \"l2\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a We create a ```GridSearchCV``` object , with a ```Pipeline```  object, as before  the parameter cv Determines the cross-validation splitting strategy, we use the predefined split. Finely we use the the parameter grid defined above:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ...,  0,  0])),\n",
       "       error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('CV', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_a...penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'CV__stop_words': [['a'], ['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn'...needn']], 'CV__min_df': [1, 5, 25, 125, 625], 'CV__max_df': [0.6, 0.9], 'LR__penalty': ['l1', 'l2']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GridSearchCV(pipe, param_grid ,cv=pds)\n",
    "clf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit the dataset, as we need to perform a BOW transform in the pipeline object. We input the original dataframe of documents as an input \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ...,  0,  0])),\n",
       "       error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('CV', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_a...penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'CV__stop_words': [['a'], ['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn'...needn']], 'CV__min_df': [1, 5, 25, 125, 625], 'CV__max_df': [0.6, 0.9], 'LR__penalty': ['l1', 'l2']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(dataset['X'], y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print out the highest accuracy in the validation data and the corresponding best parameter:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "highest accuracy in the validation 0.6039963669391463\n",
      "best hyperparameters : {'CV__max_df': 0.6, 'CV__min_df': 1, 'CV__stop_words': ['a'], 'LR__penalty': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "print(\"highest accuracy in the validation\",clf.best_score_)\n",
    "print(\"best hyperparameters :\",clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " best parameters best parameters accuracy score : 0.6520361990950226\n"
     ]
    }
   ],
   "source": [
    "print(\" best parameters best parameters accuracy score :\",clf.score(test_dataset['X'],y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term Frequency–Inverse Document Frequency (TF–IDF)\n",
    "\n",
    "The goal of using TF-IDF instead of the raw frequencies of occurrence of a token in a given document is to scale down tokens that occur very frequently in a given corpus and that are hence empirically less informative than features that occur in a small fraction of the training corpus.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Toy Example Using TF–IDF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formula that is used to compute the tf-idf for a term $t$ of a document $d$ in a document set is  $tf-idf(t, d) = tf(t, d) * idf(t)$. Where tf is the term freqency matrix  and the idf is computed as $idf(t) = log [ N / df(t) ] + 1 $. Where $N$is the total number of documents in the document set and $df(t)$ is the document frequency of $t$; the document frequency is the number of documents in the document set that contain the term $t$. The effect of adding “1” to the idf in the equation above is that terms with zero idf, i.e., terms that occur in all documents in a training set, will not be entirely ignored [1]. All the terms are shown here:\n",
    "\n",
    "t — term (word)\n",
    "\n",
    "d — document (set of words)\n",
    "\n",
    "N — count of corpus\n",
    "\n",
    "corpus — the total document set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert the TFM to a TF–IDF; let's compare the two to have a better idea of what's going on using the toy example. We determine TFM and use the TfidfTransformer  class to recalculate the TF–IDF and plot both as a dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is the first of document .',\n",
       " 'This document is the second document.',\n",
       " 'And this is the third one.',\n",
       " 'Is this the first document?']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vectorizer=CountVectorizer()\n",
    "X_toy = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>document</th>\n",
       "      <th>first</th>\n",
       "      <th>is</th>\n",
       "      <th>of</th>\n",
       "      <th>one</th>\n",
       "      <th>second</th>\n",
       "      <th>the</th>\n",
       "      <th>third</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>This is the first of document .</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>This document is the second document.</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>And this is the third one.</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Is this the first document?</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       and  document  first  is  of  one  \\\n",
       "This is the first of document .          0         1      1   1   1    0   \n",
       "This document is the second document.    0         2      0   1   0    0   \n",
       "And this is the third one.               1         0      0   1   0    1   \n",
       "Is this the first document?              0         1      1   1   0    0   \n",
       "\n",
       "                                       second  the  third  this  \n",
       "This is the first of document .             0    1      0     1  \n",
       "This document is the second document.       1    1      0     1  \n",
       "And this is the third one.                  0    1      1     1  \n",
       "Is this the first document?                 0    1      0     1  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_dataframe=pd.DataFrame(X_toy.toarray(),index=corpus,columns=vectorizer.get_feature_names())\n",
    "tf_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>document</th>\n",
       "      <th>first</th>\n",
       "      <th>is</th>\n",
       "      <th>of</th>\n",
       "      <th>one</th>\n",
       "      <th>second</th>\n",
       "      <th>the</th>\n",
       "      <th>third</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>This is the first of document .</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.378357</td>\n",
       "      <td>0.467346</td>\n",
       "      <td>0.309332</td>\n",
       "      <td>0.592769</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.309332</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.309332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>This document is the second document.</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.687624</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.281089</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538648</td>\n",
       "      <td>0.281089</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.281089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>And this is the third one.</th>\n",
       "      <td>0.511849</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.267104</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.511849</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.267104</td>\n",
       "      <td>0.511849</td>\n",
       "      <td>0.267104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Is this the first document?</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.469791</td>\n",
       "      <td>0.580286</td>\n",
       "      <td>0.384085</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.384085</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.384085</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            and  document     first        is  \\\n",
       "This is the first of document .        0.000000  0.378357  0.467346  0.309332   \n",
       "This document is the second document.  0.000000  0.687624  0.000000  0.281089   \n",
       "And this is the third one.             0.511849  0.000000  0.000000  0.267104   \n",
       "Is this the first document?            0.000000  0.469791  0.580286  0.384085   \n",
       "\n",
       "                                             of       one    second       the  \\\n",
       "This is the first of document .        0.592769  0.000000  0.000000  0.309332   \n",
       "This document is the second document.  0.000000  0.000000  0.538648  0.281089   \n",
       "And this is the third one.             0.000000  0.511849  0.000000  0.267104   \n",
       "Is this the first document?            0.000000  0.000000  0.000000  0.384085   \n",
       "\n",
       "                                          third      this  \n",
       "This is the first of document .        0.000000  0.309332  \n",
       "This document is the second document.  0.000000  0.281089  \n",
       "And this is the third one.             0.511849  0.267104  \n",
       "Is this the first document?            0.000000  0.384085  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(TfidfTransformer().fit_transform(tf_dataframe).toarray(),index=corpus,columns=vectorizer.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the TFD to TF-IDF, we see words that occur in all documents, like \"documents\" and \"is,\" are scaled down much more relatively to words like \"first,\" \"second,\" and \"third\" that only occur in one document \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF–IDF for Sentiment Analysis \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate  TF–IDF using the dataframe directly using ```TfidfVectorizer``` and then train a classifier. Many of the parameters are the same; let's create a pipeline object \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[(\"CV\", TfidfVectorizer()),  (\"LR\", LogisticRegression())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the hyperparameters of ```TfidfVectorizer``` are the same for ```CountVectorizer``` , therefore we can use the same dictionary of hyperparameters:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"CV__stop_words\":[stopwords.words('english')[0:n] for n in range(1,150,50)],\n",
    "    \"CV__min_df\":[5**n for n in range(5)],\n",
    "    \"CV__max_df\":[0.6,0.9],\n",
    "    \"LR__penalty\":[\"l1\", \"l2\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we crate a ```GridSearchCV``` object \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ...,  0,  0])),\n",
       "       error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('CV', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "  ...penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'CV__stop_words': [['a'], ['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn'...needn']], 'CV__min_df': [1, 5, 25, 125, 625], 'CV__max_df': [0.6, 0.9], 'LR__penalty': ['l1', 'l2']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GridSearchCV(pipe, param_grid ,cv=pds)\n",
    "clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we fit the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ...,  0,  0])),\n",
       "       error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('CV', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "  ...penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'CV__stop_words': [['a'], ['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn'...needn']], 'CV__min_df': [1, 5, 25, 125, 625], 'CV__max_df': [0.6, 0.9], 'LR__penalty': ['l1', 'l2']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(dataset['X'], y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we list the best parameters:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "highest accuracy in the validation 0.6276112624886467\n",
      "best hyperparameters : {'CV__max_df': 0.6, 'CV__min_df': 1, 'CV__stop_words': ['a'], 'LR__penalty': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "print(\"highest accuracy in the validation\",clf.best_score_)\n",
    "print(\"best hyperparameters :\",clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the accuracy using test data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " best parameters best parameters accuracy score : 0.6570135746606335\n"
     ]
    }
   ],
   "source": [
    "print(\" best parameters best parameters accuracy score :\",clf.score(test_dataset['X'],y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Authors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Joseph Santarcangelo](author_link) has a Ph.D. in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://nlp.stanford.edu/sentiment/index.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMGPXX08RAEN86-2022-01-01\">[1]</a>  Stanford Sentiment Treebank\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://phontron.com/class/anlp2021/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMGPXX08RAEN86-2022-01-01\">[2]</a>CMU CS 11-711 Advanced NLP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Date (YYYY-MM-DD)|Version|Changed By|Change Description|\n",
    "|-|-|-|-|\n",
    "|2020-07-17|0.1|Sam|Create Lab Template|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © 2022 IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
